{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import docx\n",
    "import string\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import spacy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function converting pdf to string\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "    \n",
    "    infile = open(fname, 'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract text from PDF\n",
    "def getPDFContent(path):\n",
    "    content = \"\"\n",
    "    # Load PDF into pyPDF\n",
    "    pdf = PyPDF2.PdfFileReader(open(path, \"rb\"))\n",
    "    # Iterate pages\n",
    "    for i in range(0, pdf.getNumPages()):\n",
    "        # Extract text from page and add to content\n",
    "        content += pdf.getPage(i).extractText() + \"\\n\"\n",
    "    # Collapse whitespace\n",
    "    content = \" \".join(content.replace(u\"\\xa0\", \" \").strip().split())\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract text from DOCX\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = \"\"\n",
    "    for para in doc.paragraphs:\n",
    "        fullText += para.text\n",
    "    return fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To store extracted resumes\n",
    "resume = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file name / path : C:\\Users\\Vikas\\Downloads\\vikas_resume (6).pdf\n",
      "processing..... \n",
      "please wait....\n"
     ]
    }
   ],
   "source": [
    "#Select a path to the file - code needs os.path #to be addded\n",
    "filename = input(\"Enter file name / path : \")\n",
    "#Invoking document parsers based on file format\n",
    "#Note: for TXT - do a normal f.read()\n",
    "if filename.endswith(\".pdf\"):\n",
    "    resume = getPDFContent(filename).encode(\"ascii\",\"ignore\") \n",
    "elif filename.endswith(\".docx\"):\n",
    "     resume = getText(filename).encode(\"ascii\", \"ignore\")  \n",
    "else:\n",
    "    print(\"File format is currently not supported\")\n",
    "    exit(0)\n",
    "\n",
    "print(\"processing..... \\nplease wait....\")\n",
    "#print(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_str=convert(r'C:\\Users\\Vikas\\Downloads\\vikas_resume (6).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resume_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing NLTK for stopword removal and tokenizing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing the given file ......\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing/ Filtering the resume off stopwords and punctuations \n",
    "print(\"tokenizing the given file ......\")\n",
    "tokens = word_tokenize(resume_str)\n",
    "punctuations = ['(',')',';',':','[',']',',']\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing the stop words....\n",
      "Cleaning the resumes....\n",
      "Extracting Text .......\n",
      "['Name', 'VIKAS', 'KUMAR', 'vikassrm9', 'gmail.com', 'Mobile', '+91-6281729732', 'CAREER', 'OBJECTIVE', 'To', 'work', 'challenging', 'atmosphere', 'provides', 'ample', 'opportunities', 'learning', 'growth', 'IT', 'Industry', 'EXPERIENCE', 'SUMMARY', 'Approx', '1.5', 'year', 'experience', 'IT', 'Industry', '\\uf0b7', '\\uf0b7', 'Junior', 'Data', 'Scientist', 'Grid', 'R', 'D.', '\\uf0b7', '\\uf0b7', 'Highly', 'skilled', 'Machine', 'learning', 'data', 'visualization', 'problem', 'solving', '\\uf0b7', 'IBM', 'CERTIFIED', 'JUNIOR', 'DATA', 'SCIENTIST', 'Skilled', 'Statistical', 'techniques', 'Linear', 'Logistic', 'regression', 'Clustering', 'Tree', 'Svm', 'Naive', 'bayes', 'classifier', 'knowledge', 'Natural', 'Language', 'Processing', 'NLP', '\\uf0b7', 'Used', 'Python/R', 'Programing', 'Analysis', 'Visualization', '\\uf0b7', 'Skilled', 'Advanced', 'Excel', '\\uf0b7', 'Experience', 'implementing', 'Machine', 'Learning', 'Models', 'using', 'ML', 'Scikit', 'Learn', '\\uf0b7', 'PYTHON', 'Numpy', 'Pandas', 'matplotlib', 'Seaborn', 'nltk', 'R', 'MACHINE', 'LEARNING', 'SQL', 'WORKING', 'EXPERIENCE', 'Junior', 'Data', 'Scientist', 'Grid', 'R', 'D', 'jan', '2019', 'present', 'TRAINING', 'EXPERIENCE', 'Software', 'Trainee', 'DAMCO', 'SOLUTIONS', 'Faridabad', 'march', '2012', 'december', '2012', 'Data', 'science', 'Python', 'R', 'Machine', 'learning', 'Advanced', 'Excel', 'IBM', 'Watson', 'Analytics', 'training', 'IBM', 'april', '2018', 'dec', '2018', 'ACADEMIC', 'QUALIFICATION', 'B.Tech', '2007-2011', 'From', 'SRM', 'Ghaziabad', '76', 'marks', '12th', 'FromS', 'N', 'S', 'C', 'Aurangabad', 'with54', 'marks', '10th', 'From', 'S', 'S', 'M', 'Aurangabad', '68.11', 'marks', 'TECHNICAL', 'SKILLS', 'Programming', 'Languages', 'IDE', '\\uf0b7', '\\uf0b7', '\\uf0b7', 'DBMS', '\\uf0b7', 'OTHER', 'PYTHON', 'R', 'JUPYTER', 'NOTEBOOK', 'SQL', 'IBM', 'Watson', 'Analytics', 'Machine', 'Learning', 'PROJECTS', 'UNDERTAKEN', 'Project1', 'Title', 'Organization', 'Environment', 'Suv', 'car', 'data', 'analysis', 'Pace', 'hyderabad', 'Python', 'Machine', 'learning', 'Project', 'details', 'A', 'car', 'company', 'released', 'new', 'suv', 'market', 'using', 'previous', 'data', 'sales', 'suv', \"'s\", 'want', 'predict', 'category', 'people', 'might', 'interested', 'buying', 'Project', 'uploaded', 'Github', 'https', '//github.com/vikassrm', 'Responsibilities', '•', '•', '•', '•', 'Understanding', 'Analysis', 'Module', 'Involved', 'creating', 'machine', 'learning', 'model', 'Missing', 'value', 'fixing', 'Report', 'generation', 'python', 'Graph', 'KEY', 'ACHIEVMENT', '\\uf0b7', 'Successfully', 'organized', 'event', '‘', 'LAN', 'GAMING', '’', '‘', 'RUBAROO', '’', '-National', 'level', 'cultural', 'fest', 'SRM', 'University', 'Successfully', 'organized', 'seminar', '‘', 'MICROSOFT', 'DREAMSPARK', 'YATRA', '’', 'SRM', '\\uf0b7', 'Certificate', 'excellence', '–', 'general', 'study', 'Akhil', 'bharti', 'shiksha', 'sansthan', '\\uf0b7', '\\uf0b7', 'Certificate', 'Participation', '--', 'Data', 'science', 'Capgemini', 'Tech', 'Challenge', '2018', '\\uf0b7', 'Certificate', 'Completion—Machine', 'learning', 'Trading', 'Quantra', 'KEY', 'SKILLS', 'Positive', 'attitude', 'Team', 'worker', 'Willingness', 'accept', 'challengeable', 'responsibilities', 'LANGUAGES', 'KNOWN', 'English', '\\uf0b7', '\\uf0b7', 'Hindi', '\\uf0b7', 'French', 'basic', 'PERSONAL', 'INFORMATION', '\\uf0b7', 'Father', '’', 'Name', 'Mr.', 'Basant', 'Narayan', '\\uf0b7', 'Permanent', 'Address', 'South', 'town', 'school', '\\uf0b7', 'Date', 'Birth', '\\uf0b7', 'Marital', 'Status', '\\uf0b7', 'Nationality', 'Gandhinagar', 'Aurangabad', 'Bihar-824101', '15thJanuary', '1990', 'Single', 'Indian', 'DECLARATION', 'I', 'hereby', 'declare', 'information', 'true', 'best', 'knowledge', 'Place', 'Bangalore', 'Date', '__________', 'Name', 'candidate', 'Vikas', 'Kumar']\n"
     ]
    }
   ],
   "source": [
    "#storing the cleaned resume\n",
    "filtered = [w for w in tokens if not w in stop_words and  not w in string.punctuation]\n",
    "print(\"removing the stop words....\\nCleaning the resumes....\\nExtracting Text .......\")\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name : VIKAS KUMAR\n"
     ]
    }
   ],
   "source": [
    "#get the name from the resume\n",
    "name  = str(filtered[1])+' '   +str(filtered[2])\n",
    "print(\"Name : \" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email : vikassrm9@gmail.com\n"
     ]
    }
   ],
   "source": [
    "#using regular expressions we extract phone numbers and mail ids\n",
    "import re\n",
    "#get contact info - from resume\n",
    "#email\n",
    "email = \"\"\n",
    "match_mail = re.search(r'[\\w\\.-]+@[\\w\\.-]+', resume_str)\n",
    "#handling the cases when mobile number is not given\n",
    "if(match_mail != None):\n",
    "    email = match_mail.group(0)\n",
    "print(\"Email : \" + email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobile : 628172973\n"
     ]
    }
   ],
   "source": [
    "#mobile number\n",
    "mobile = \"\"\n",
    "match_mobile = re.search(r'((?:\\(?\\+91\\)?)?\\d{9})',resume_str)\n",
    "#handling the cases when mobile number is not given\n",
    "if(match_mobile != None):\n",
    "    mobile = match_mobile.group(0)\n",
    "print(\"Mobile : \" +  mobile)\n",
    "\n",
    "#parsed_resume = ' '.join(filtered)\n",
    "#print(\"Parsed Resume in plain Text : \", parsed_resume)\n",
    "#r = str(parsed_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
